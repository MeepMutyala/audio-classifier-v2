{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 1: Setup and imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "from vjepa2_audio import VJEPA2AudioClassifier\n",
        "from audio_utils import create_dataloaders\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "print(\"üöÄ V-JEPA2 Audio Classifier Training\")\n",
        "print(\"=\" * 50)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 2: Configuration\n",
        "config = {\n",
        "    'model_type': 'tubelet',  # V-JEPA2 uses tubelet format\n",
        "    'batch_size': 8,          # V-JEPA2 is memory intensive\n",
        "    'learning_rate': 1e-4,\n",
        "    'epochs': 100,\n",
        "    'patience': 10,\n",
        "    'n_mels': 128,\n",
        "    'num_classes': 50,\n",
        "    'img_size': (128, 157),   # (n_mels, time_frames)\n",
        "    'patch_size': 16,\n",
        "    'num_frames': 16,\n",
        "    'tubelet_size': 2,\n",
        "    'embed_dim': 384,\n",
        "    'depth': 8,\n",
        "    'num_heads': 8\n",
        "}\n",
        "\n",
        "print(\"üìã Configuration:\")\n",
        "for key, value in config.items():\n",
        "    print(f\"  {key}: {value}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 3: Create data loaders (with tubelet format)\n",
        "print(\"üìÅ Creating data loaders...\")\n",
        "train_loader, val_loader, test_loader, num_classes = create_dataloaders(\n",
        "    model_type='tubelet',     # Different from sequence models!\n",
        "    batch_size=config['batch_size'],\n",
        "    num_workers=2,\n",
        "    augment=True,\n",
        "    augment_factor=2\n",
        ")\n",
        "\n",
        "print(f\"‚úÖ Data loaded: {len(train_loader)} train, {len(val_loader)} val, {len(test_loader)} test batches\")\n",
        "print(f\"üìä Classes: {num_classes}\")\n",
        "\n",
        "# Test data loading\n",
        "sample_batch = next(iter(train_loader))\n",
        "sample_data, sample_labels = sample_batch\n",
        "print(f\"üìä Sample batch shape: {sample_data.shape}\")\n",
        "print(f\"üìä Sample labels: {sample_labels[:5]}\")\n",
        "print(f\"üìä Expected tubelet format: [batch, channels, time, freq, context]\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 4: Create model\n",
        "print(\"üîß Creating V-JEPA2 model...\")\n",
        "model = VJEPA2AudioClassifier(\n",
        "    num_classes=num_classes,\n",
        "    img_size=config['img_size'],\n",
        "    patch_size=config['patch_size'],\n",
        "    num_frames=config['num_frames'],\n",
        "    tubelet_size=config['tubelet_size'],\n",
        "    embed_dim=config['embed_dim'],\n",
        "    depth=config['depth'],\n",
        "    num_heads=config['num_heads']\n",
        ").to(device)\n",
        "\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"‚úÖ V-JEPA2 model created: {total_params:,} parameters\")\n",
        "\n",
        "# Test forward pass\n",
        "dummy_batch = next(iter(train_loader))\n",
        "dummy_input, _ = dummy_batch\n",
        "dummy_input = dummy_input.to(device)\n",
        "with torch.no_grad():\n",
        "    output = model(dummy_input)\n",
        "print(f\"‚úÖ Forward pass test: {dummy_input.shape} -> {output.shape}\")\n",
        "\n",
        "# Model summary\n",
        "print(f\"\\nüìä Model Architecture:\")\n",
        "print(f\"  Input: [batch, channels, time, freq, context]\")\n",
        "print(f\"  Embedding: {config['embed_dim']} dimensions\")\n",
        "print(f\"  Output: [batch, {num_classes}]\")\n",
        "print(f\"  Parameters: {total_params:,}\")\n",
        "print(f\"  Vision Transformer: {config['depth']} layers, {config['num_heads']} heads\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 5: Training setup\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.AdamW(model.parameters(), lr=config['learning_rate'], weight_decay=0.01)\n",
        "scheduler = CosineAnnealingLR(optimizer, T_max=config['epochs'])\n",
        "\n",
        "# Training tracking\n",
        "train_losses = []\n",
        "val_accuracies = []\n",
        "best_val_acc = 0.0\n",
        "patience_counter = 0\n",
        "\n",
        "print(\"üöÄ Starting training...\")\n",
        "print(f\"üìä Training setup:\")\n",
        "print(f\"  Loss: CrossEntropyLoss\")\n",
        "print(f\"  Optimizer: AdamW (lr={config['learning_rate']}, wd=0.01)\")\n",
        "print(f\"  Scheduler: CosineAnnealingLR\")\n",
        "print(f\"  Early stopping: {config['patience']} epochs\")\n",
        "print(f\"  Note: V-JEPA2 uses tubelet format for audio processing\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 6: Training loop\n",
        "start_time = time.time()\n",
        "\n",
        "for epoch in range(config['epochs']):\n",
        "    # Training phase\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    \n",
        "    with tqdm(train_loader, desc=f'Epoch {epoch+1}/{config[\"epochs\"]}') as pbar:\n",
        "        for batch_idx, (data, targets) in enumerate(pbar):\n",
        "            data, targets = data.to(device), targets.to(device)\n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(data)\n",
        "            loss = criterion(outputs, targets)\n",
        "            loss.backward()\n",
        "            \n",
        "            # Gradient clipping for stability\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            \n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "            \n",
        "            pbar.set_postfix({'Loss': f'{loss.item():.4f}'})\n",
        "    \n",
        "    avg_train_loss = total_loss / len(train_loader)\n",
        "    train_losses.append(avg_train_loss)\n",
        "    \n",
        "    # Validation phase\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for data, targets in val_loader:\n",
        "            data, targets = data.to(device), targets.to(device)\n",
        "            outputs = model(data)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += targets.size(0)\n",
        "            correct += (predicted == targets).sum().item()\n",
        "    \n",
        "    val_acc = correct / total\n",
        "    val_accuracies.append(val_acc)\n",
        "    \n",
        "    scheduler.step()\n",
        "    \n",
        "    print(f'Epoch {epoch+1:3d} | Loss: {avg_train_loss:.4f} | Val Acc: {val_acc:.4f}')\n",
        "    \n",
        "    # Early stopping\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        patience_counter = 0\n",
        "        torch.save(model.state_dict(), 'best_vjepa2_model.pth')\n",
        "        print(f'üíæ New best model saved! Val Acc: {val_acc:.4f}')\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "        if patience_counter >= config['patience']:\n",
        "            print(f'üõë Early stopping after {epoch+1} epochs')\n",
        "            break\n",
        "\n",
        "training_time = time.time() - start_time\n",
        "print(f\"‚è±Ô∏è Training completed in {training_time:.2f} seconds\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 7: Test evaluation\n",
        "print(\"üß™ Evaluating on test set...\")\n",
        "model.load_state_dict(torch.load('best_vjepa2_model.pth'))\n",
        "model.eval()\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "test_predictions = []\n",
        "test_targets = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for data, targets in test_loader:\n",
        "        data, targets = data.to(device), targets.to(device)\n",
        "        outputs = model(data)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += targets.size(0)\n",
        "        correct += (predicted == targets).sum().item()\n",
        "        \n",
        "        test_predictions.extend(predicted.cpu().numpy())\n",
        "        test_targets.extend(targets.cpu().numpy())\n",
        "\n",
        "test_acc = correct / total\n",
        "print(f'üéØ Final Test Accuracy: {test_acc:.4f}')\n",
        "\n",
        "# Calculate per-class accuracy\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import seaborn as sns\n",
        "\n",
        "print(\"\\nüìä Per-class Performance:\")\n",
        "print(classification_report(test_targets, test_predictions, target_names=[f'Class_{i}' for i in range(num_classes)]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 8: Results summary and visualization\n",
        "print(\"\\nüìä TRAINING SUMMARY\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"Model: V-JEPA2 Audio Classifier\")\n",
        "print(f\"Parameters: {total_params:,}\")\n",
        "print(f\"Best Validation Accuracy: {best_val_acc:.4f}\")\n",
        "print(f\"Final Test Accuracy: {test_acc:.4f}\")\n",
        "print(f\"Training Time: {training_time:.2f} seconds\")\n",
        "print(f\"Training completed!\")\n",
        "\n",
        "# Plot training curves\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "# Loss curve\n",
        "ax1.plot(train_losses, label='Training Loss', color='blue')\n",
        "ax1.set_xlabel('Epoch')\n",
        "ax1.set_ylabel('Loss')\n",
        "ax1.set_title('Training Loss')\n",
        "ax1.legend()\n",
        "ax1.grid(True)\n",
        "\n",
        "# Accuracy curve\n",
        "ax2.plot(val_accuracies, label='Validation Accuracy', color='green')\n",
        "ax2.axhline(y=best_val_acc, color='red', linestyle='--', label=f'Best: {best_val_acc:.4f}')\n",
        "ax2.set_xlabel('Epoch')\n",
        "ax2.set_ylabel('Accuracy')\n",
        "ax2.set_title('Validation Accuracy')\n",
        "ax2.legend()\n",
        "ax2.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Confusion Matrix\n",
        "cm = confusion_matrix(test_targets, test_predictions)\n",
        "plt.figure(figsize=(12, 10))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
        "            xticklabels=[f'C{i}' for i in range(num_classes)],\n",
        "            yticklabels=[f'C{i}' for i in range(num_classes)])\n",
        "plt.title('Confusion Matrix - V-JEPA2 Audio Classifier')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
